# Biases to acknowledge

- We tested only one input variable  .

# Pre-implementation

## 1. Recommended Implementation Roadmap (Feasible & Smart)
C:.
â”œâ”€â”€â”€rag_phase0
â”‚   â”‚   check.py
â”‚   â”‚   config.py
â”‚   â”‚   requirements.txt
â”‚   â”‚
â”‚   â”œâ”€â”€â”€data
â”‚   â”‚   â”‚   chunks.pkl
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€â”€pdfs
â”‚   â”‚           Basics_of_math.pdf
â”‚   â”‚
â”‚   â”œâ”€â”€â”€db
â”‚   â”‚       schema.sql
â”‚   â”‚
â”‚   â”œâ”€â”€â”€faiss
â”‚   â”‚       index.faiss
â”‚   â”‚
â”‚   â”œâ”€â”€â”€scripts
â”‚   â”‚       01_init_db.py
â”‚   â”‚       02_chunk_pdf.py
â”‚   â”‚       03_embed_and_index.py
â”‚   â”‚       04_test_retrieval.py
â”‚   â”‚
â”‚   â””â”€â”€â”€__pycache__
â”‚           config.cpython-313.pyc
â”‚
â”œâ”€â”€â”€rag_phase1
â”‚   â”‚   check.py
â”‚   â”‚   config.py
â”‚   â”‚   requirements.txt
â”‚   â”‚
â”‚   â”œâ”€â”€â”€data
â”‚   â”‚   â”‚   chunks.pkl
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€â”€pdfs
â”‚   â”‚           Basics_of_math.pdf
â”‚   â”‚
â”‚   â”œâ”€â”€â”€db
â”‚   â”‚       schema.sql
â”‚   â”‚
â”‚   â”œâ”€â”€â”€faiss
â”‚   â”‚       index.faiss
â”‚   â”‚
â”‚   â”œâ”€â”€â”€scripts
â”‚   â”‚   â”‚   content_generator.py
â”‚   â”‚   â”‚   orchestrator.py
â”‚   â”‚   â”‚   planning_agent.py
â”‚   â”‚   â”‚   profile_agent.py
â”‚   â”‚   â”‚   xai_agent.py
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€â”€__pycache__
â”‚   â”‚           content_generator.cpython-313.pyc
â”‚   â”‚           planning_agent.cpython-313.pyc
â”‚   â”‚           profile_agent.cpython-313.pyc
â”‚   â”‚           xai_agent.cpython-313.pyc
â”‚   â”‚
â”‚   â””â”€â”€â”€__pycache__
â”‚           config.cpython-313.pyc
â”‚
â”œâ”€â”€â”€rag_phase2
â”‚   â”‚   check.py
â”‚   â”‚   config.py
â”‚   â”‚   requirements.txt
â”‚   â”‚
â”‚   â”œâ”€â”€â”€data
â”‚   â”‚   â”‚   chunks.pkl
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€â”€pdfs
â”‚   â”‚           Basics_of_math.pdf
â”‚   â”‚
â”‚   â”œâ”€â”€â”€db
â”‚   â”‚       schema_phase2.sql
â”‚   â”‚
â”‚   â”œâ”€â”€â”€faiss
â”‚   â”‚       index.faiss
â”‚   â”‚
â”‚   â”œâ”€â”€â”€scripts
â”‚   â”‚       01_simulate_learners.py
â”‚   â”‚       02_log_interactions.py
â”‚   â”‚       03_hybrid_recommender.py
â”‚   â”‚       04_explain_ranking.py
â”‚   â”‚       05_evaluate.py
â”‚   â”‚
â”‚   â””â”€â”€â”€__pycache__
â”‚           config.cpython-313.pyc
â”‚
â””â”€â”€â”€rag_phase3
    â”‚   check.py
    â”‚   config.py
    â”‚   requirements.txt
    â”‚   shap_values.npy
    â”‚
    â”œâ”€â”€â”€data
    â”‚   â”‚   chunks.pkl
    â”‚   â”‚
    â”‚   â””â”€â”€â”€pdfs
    â”‚           Basics_of_math.pdf
    â”‚
    â”œâ”€â”€â”€db
    â”‚       schema.sql
    â”‚       schema_phase2.sql
    â”‚
    â”œâ”€â”€â”€faiss
    â”‚       index.faiss
    â”‚
    â”œâ”€â”€â”€scripts
    â”‚       06_shap_analysis.py
    â”‚       07_evaluate_xai.py
    â”‚       08_bias_audit.py
    â”‚       xai_agent.py
    â”‚
    â””â”€â”€â”€__pycache__
            config.cpython-313.pyc

### ğŸ”¹ Phase 0 â€“ Dataset & Knowledge Base (Week 1)
What you ACTUALLY do, code-wise

Iâ€™ll structure this as:
Project structure (folders & files)
One-time setup (Postgres + dependencies)
Data ingestion pipeline (PDF â†’ chunks â†’ embeddings â†’ DB)
Retrieval test (prove it works)
No agents. No overengineering.
1ï¸âƒ£ Project Structure (DO THIS FIRST)

Create a simple Python project:

rag_phase0/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ pdfs/
â”‚   â”‚   â””â”€â”€ quadratic_eq.pdf
â”‚
â”œâ”€â”€ db/
â”‚   â””â”€â”€ schema.sql
â”‚
â”œâ”€â”€ faiss/
â”‚   â””â”€â”€ index.faiss
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ 01_init_db.py
â”‚   â”œâ”€â”€ 02_chunk_pdf.py
â”‚   â”œâ”€â”€ 03_embed_and_index.py
â”‚   â””â”€â”€ 04_test_retrieval.py
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ config.py

This keeps everything mentally tractable.
2ï¸âƒ£ One-Time Setup
A. Install dependencies
requirements.txt
sentence-transformers
faiss-cpu
psycopg2-binary
pdfplumber
numpy
pip install -r requirements.txt
B. PostgreSQL schema (RUN ONCE)
db/schema.sql
CREATE TABLE courses (
Â  id SERIAL PRIMARY KEY,
Â  name TEXT,
Â  domain TEXT
);
CREATE TABLE concepts (
Â  id SERIAL PRIMARY KEY,
Â  name TEXT,
Â  course_id INT,
Â  difficulty TEXT,
Â  prerequisites INT[]
);
CREATE TABLE resources (
Â  id SERIAL PRIMARY KEY,
Â  type TEXT,
Â  source TEXT,
Â  course_id INT
);
CREATE TABLE chunks (
Â  id SERIAL PRIMARY KEY,
Â  resource_id INT,
Â  concept_id INT,
Â  chunk_type TEXT,
Â  difficulty TEXT,
Â  faiss_index INT,
Â  text TEXT
);
Create DB and apply schema:
createdb ragdb
psql ragdb -f db/schema.sql
C. DB config
config.py
DB_CONFIG = {
    "dbname": "ragdb",
    "user": "postgres",
    "password": "123",
    "host": "localhost",
    "port": 5432
}
3ï¸âƒ£ Data Ingestion Pipeline (THIS IS THE CORE)
ğŸ”¹ Script 01 â€” Initialize Metadata
scripts/01_init_db.py
import os
import sys
âœ… Add project root (rag_phase0) to Python path FIRST
PROJECT_ROOT = os.path.dirname(os.path.dirname(__file__))
sys.path.insert(0, PROJECT_ROOT)
import psycopg2
from config import DB_CONFIG
conn = psycopg2.connect(**DB_CONFIG)
cur = conn.cursor()
cur.execute("""
INSERT INTO courses (name, domain)
VALUES ('Linear & Quadratic Equations', 'Mathematics')
RETURNING id;
""")
course_id = cur.fetchone()[0]
cur.execute("""
INSERT INTO concepts (name, course_id, difficulty)
VALUES ('quadratic_equations', %s, 'beginner')
RETURNING id;
""", (course_id,))
concept_id = cur.fetchone()[0]
cur.execute("""
INSERT INTO resources (type, source, course_id)
VALUES ('pdf', 'quadratic_eq.pdf', %s)
RETURNING id;
""", (course_id,))
resource_id = cur.fetchone()[0]
conn.commit()
conn.close()
print(course_id, concept_id, resource_id)
âœ… Output: you now have IDs to attach chunks to.

ğŸ”¹ Script 02 â€” Chunk the PDF
scripts/02_chunk_pdf.py
import pdfplumber
import re
import pickle
import os
PDF_PATH = "data/pdfs/Basics_of_math.pdf"
CHUNKS_PATH = "data/chunks.pkl"
def detect_chunk_type(text):
Â  Â  if re.search(r"example", text, re.I):
Â  Â  Â  Â  return "example"
Â  Â  if re.search(r"exercise", text, re.I):
Â  Â  Â  Â  return "exercise"
Â  Â  if re.search(r"solution", text, re.I):
Â  Â  Â  Â  return "solution"
Â  Â  return "explanation"
chunks = []
with pdfplumber.open(PDF_PATH) as pdf:
Â  Â  for page in pdf.pages:
Â  Â  Â  Â  text = page.extract_text()
Â  Â  Â  Â  if not text:
Â  Â  Â  Â  Â  Â  continue
Â  Â  Â  Â  sections = text.split("\n\n")
Â  Â  Â  Â  for sec in sections:
Â  Â  Â  Â  Â  Â  if len(sec.split()) < 50:
Â  Â  Â  Â  Â  Â  Â  Â  continue
Â  Â  Â  Â  Â  Â  chunks.append({
Â  Â  Â  Â  Â  Â  Â  Â  "text": sec.strip(),
Â  Â  Â  Â  Â  Â  Â  Â  "chunk_type": detect_chunk_type(sec),
Â  Â  Â  Â  Â  Â  Â  Â  "difficulty": "beginner"
Â  Â  Â  Â  Â  Â  })
print(f"Extracted {len(chunks)} chunks")
 Save chunks to disk
os.makedirs(os.path.dirname(CHUNKS_PATH), exist_ok=True)
with open(CHUNKS_PATH, "wb") as f:
Â  Â  pickle.dump(chunks, f)

print(f"Saved chunks to {CHUNKS_PATH}")

âœ… Result: a list of pedagogical chunks, not raw text.

ğŸ”¹ Script 03 â€” Embed + Store in FAISS + PostgreSQL
scripts/03_embed_and_index.py
import psycopg2
import numpy as np
import faiss
from sentence_transformers import SentenceTransformer
import pickle
import os
import sys
PROJECT_ROOT = os.path.dirname(os.path.dirname(__file__))
sys.path.insert(0, PROJECT_ROOT)
from config import DB_CONFIG
CHUNKS_PATH = "data/chunks.pkl"
Load chunks
with open(CHUNKS_PATH, "rb") as f:
Â  Â  chunks = pickle.load(f)
 IDs from step 01
RESOURCE_ID = 1
CONCEPT_ID = 1
model = SentenceTransformer("all-MiniLM-L6-v2")
texts = [c["text"] for c in chunks]
embeddings = model.encode(texts)
dim = embeddings.shape[1]
index = faiss.IndexFlatL2(dim)
index.add(np.array(embeddings, dtype=np.float32)) Â # FAISS requires float32
Save FAISS index
os.makedirs("faiss", exist_ok=True)
faiss.write_index(index, "faiss/index.faiss")
Insert into PostgreSQL
conn = psycopg2.connect(**DB_CONFIG)
cur = conn.cursor()
for i, chunk in enumerate(chunks):
Â  Â  cur.execute("""
Â  Â  Â  Â  INSERT INTO chunks
Â  Â  Â  Â  (resource_id, concept_id, chunk_type, difficulty, faiss_index, text)
Â  Â  Â  Â  VALUES (%s, %s, %s, %s, %s, %s)
Â  Â  """, (
Â  Â  Â  Â  RESOURCE_ID,
Â  Â  Â  Â  CONCEPT_ID,
Â  Â  Â  Â  chunk["chunk_type"],
Â  Â  Â  Â  chunk["difficulty"],
Â  Â  Â  Â  i,
Â  Â  Â  Â  chunk["text"] Â # <-- new
Â  Â  ))
conn.commit()
conn.close()
print("FAISS + PostgreSQL populated")
âœ… This is the key moment:
FAISS holds vectors
PostgreSQL holds structure
faiss_index links both
4ï¸âƒ£ Retrieval Test (Prove Phase 0 Works)

scripts/04_test_retrieval.py
import faiss
imort psycopg2
import numpy as np
from sentence_transformers import SentenceTransformer
import os
import sys
PROJECT_ROOT = os.path.dirname(os.path.dirname(__file__))
sys.path.insert(0, PROJECT_ROOT)
from config import DB_CONFIG
model = SentenceTransformer("all-MiniLM-L6-v2")
index = faiss.read_index("faiss/index.faiss")
query = "Give an example of quadratic equations"
q_emb = model.encode([query])
D, I = index.search(np.array(q_emb), k=5)
conn = psycopg2.connect(**DB_CONFIG)
cur = conn.cursor()
for idx in I[0]:
Â  Â  cur.execute("""
Â  Â  Â  Â  SELECT chunk_type, difficulty
Â  Â  Â  Â  FROM chunks
Â  Â  Â  Â  WHERE faiss_index = %s
Â  Â  """, (int(idx),))
Â  Â  print(cur.fetchone())
conn.close()
If you see:
('example', 'beginner')
ğŸ‰ Phase 0 is DONE.

#### Terminal
C:\Users\hatta\Desktop\study\M2\Generative AI\Projet\Implementation\rag_phase0>createdb -U postgres ragdb
Password:

createdb: error: database creation failed: ERROR:  database "ragdb" already exists

C:\Users\hatta\Desktop\study\M2\Generative AI\Projet\Implementation\rag_phase0>psql -U postgres -d ragdb -f db/schema.sql
Password for user postgres:

CREATE TABLE
CREATE TABLE
CREATE TABLE
CREATE TABLE

C:\Users\hatta\Desktop\study\M2\Generative AI\Projet\Implementation\rag_phase0>psql -d ragdb -U postgres
Password for user postgres:

psql (18.1)
WARNING: Console code page (850) differs from Windows code page (1252)
         8-bit characters might not work correctly. See psql reference
         page "Notes for Windows users" for details.
Type "help" for help.

ragdb=# psql -U postgres -d ragdb
ragdb-# \dt
            List of tables
 Schema |   Name    | Type  |  Owner
--------+-----------+-------+----------
 public | chunks    | table | postgres
 public | concepts  | table | postgres
 public | courses   | table | postgres
 public | resources | table | postgres
(4 rows)

ragdb-# \dt\q

C:\Users\hatta\Desktop\study\M2\Generative AI\Projet\Implementation\rag_phase0>python scripts\01_init_db.py
1 1 1

C:\Users\hatta\Desktop\study\M2\Generative AI\Projet\Implementation\rag_phase0>python scripts\02_chunk_pdf.py
Extracted 460 chunks
Saved chunks to data/chunks.pkl

C:\Users\hatta\Desktop\study\M2\Generative AI\Projet\Implementation\rag_phase0>python scripts\03_embed_and_index.py
2025-12-22 13:37:54.435921: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-12-22 13:37:56.102595: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
WARNING:tensorflow:From C:\Users\hatta\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.

FAISS + PostgreSQL populated

C:\Users\hatta\Desktop\study\M2\Generative AI\Projet\Implementation\rag_phase0>python scripts\04_test_retrieval.py
2025-12-22 13:38:31.354377: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-12-22 13:38:32.923188: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
WARNING:tensorflow:From C:\Users\hatta\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.

('example', 'beginner')
('solution', 'beginner')
('example', 'beginner')
('exercise', 'beginner')
('exercise', 'beginner')
### ğŸ”¹ Phase 1 â€“ Minimal Multi-Agent MVP (Weeks 2â€“3)
**Goal:** Demonstrate a fully working end-to-end pipeline with:

1. Learner profiling
    
2. Planning a learning path
    
3. Orchestrator managing agents
    
4. Content generation via RAG + LLM
    
5. Explainable recommendations (XAI)

We'll build a **simplified version first**, just enough for a demo scenario:

> Beginner learner fails quiz â†’ system adapts â†’ explains why

---

#### **1ï¸âƒ£ Learner Profile Agent**
**Purpose:** Build a **structured learner profile** from interaction data.

- Input: Quiz responses, activity logs
    
- Output: Learner profile object, e.g.:

`learner_profile = {     "student_id": 1,     "knowledge": {"quadratic_equations": 0.4, "linear_equations": 0.9},  # 0=low,1=high     "learning_style": "visual",     "preferences": ["examples", "step-by-step explanations"] }`

- Minimal Implementation (rule-based + simple clustering):
    
import numpy as np
def profile_learner(quiz_scores):
Â  Â  profile = {
Â  Â  Â  Â  "knowledge": quiz_scores,
Â  Â  Â  Â  "learning_style": "visual",
Â  Â  Â  Â  "preferences": ["examples", "exercises"]
Â  Â  }

#### **2ï¸âƒ£ Planning Agent**
**Purpose:** Given a learner profile, decide **next concept or resource**.

- Input: Learner profile + course concept graph
    
- Output: Learning path node(s)

Simplified demo logic:

`def plan_learning_path(profile, concepts):     # Pick weakest concept     weakest = min(profile["knowledge"], key=profile["knowledge"].get)     return {"next_concept": weakest, "recommendation_type": "example"}`

> In full version: use graph search on prerequisite graph for sequencing.

---

#### **3ï¸âƒ£ Orchestrator Agent**
**Purpose:** Central coordinator â€” calls profiling, planning, content generation, XAI.

- For Phase 1 demo, simple sequential orchestration:
    
from profile_agent import profile_learner

from planning_agent import plan_learning_path

from content_generator import generate_content

from xai_agent import justify_recommendation

def orchestrator(quiz_scores, concepts):

Â  Â  profile = profile_learner(quiz_scores)

Â  Â  plan = plan_learning_path(profile, concepts)

Â  Â  content = generate_content(plan, profile)

Â  Â  explanation = justify_recommendation(plan, profile)

Â  Â  return content, explanation

if __name__ == "__main__":

Â  Â  quiz_scores = {"linear_equations": 0.9, "quadratic_equations": 0.3}

Â  Â  concepts = ["linear_equations", "quadratic_equations"]

Â  Â  content, explanation = orchestrator(quiz_scores, concepts)

Â  Â  print("=== Recommended Content ===")

Â  Â  print(content)

Â  Â  print("\n=== Explanation ===")

Â  Â  print(explanation)

---

#### **4ï¸âƒ£ Content Generator Agent (RAG + LLM)**
- Input: Concept + learner profile
    
- Use FAISS + PostgreSQL knowledge base (from Phase 0) to **retrieve relevant chunks**
    
- Feed into LLM for natural output

Example simplified function:
import faiss

import numpy as np

import psycopg2

from sentence_transformers import SentenceTransformer

import os

import sys

PROJECT_ROOT = os.path.dirname(os.path.dirname(__file__))

sys.path.insert(0, PROJECT_ROOT)

from config import DB_CONFIG

model = SentenceTransformer("all-MiniLM-L6-v2")

index = faiss.read_index("faiss/index.faiss")

conn = psycopg2.connect(**DB_CONFIG)

cur = conn.cursor()
def generate_content(plan, profile):
Â  Â  query = f"Give a {profile['preferences'][0]} for {plan['next_concept']}"
Â  Â  q_emb = model.encode([query])
Â  Â  D, I = index.search(np.array(q_emb), k=3)
Â  Â  retrieved_texts = []
Â  Â  for idx in I[0]:
Â  Â  Â  Â  cur.execute("SELECT text FROM chunks WHERE faiss_index=%s", (int(idx),))
Â  Â  Â  Â  row = cur.fetchone()
Â  Â  Â  Â  if row:
Â  Â  Â  Â  Â  Â  retrieved_texts.append(row[0])
Â  Â  return "\n\n".join(retrieved_texts)

#### **5ï¸âƒ£ XAI / Justification Agent**
- Input: Plan + learner profile + selected content
    
- Output: Natural-language explanation of why content was recommended

Example (simplified):

def justify_recommendation(plan, profile):

Â  Â  return (

Â  Â  Â  Â  f"We recommend a {plan['recommendation_type']} for {plan['next_concept']} "

Â  Â  Â  Â  f"because your current knowledge score is {profile['knowledge'][plan['next_concept']]:.1f} "

Â  Â  Â  Â  f"and you prefer {profile['preferences'][0]}."

Â  Â  )
---

#### **6ï¸âƒ£ Demo Run**
`# Example: beginner failed quiz quiz_scores = {"linear_equations": 0.9, "quadratic_equations": 0.3} concepts = ["linear_equations", "quadratic_equations"]  content, explanation = orchestrator(quiz_scores, concepts)  print("=== Recommended Content ===") print(content) print("\n=== Explanation ===") print(explanation)`

Expected output (Phase 1 MVP):

`=== Recommended Content === [retrieved example chunks for quadratic_equations]  === Explanation === We recommend a example for quadratic_equations because your current knowledge score is 0.3 and you prefer examples.`

---

âœ… This gives you:

- **Profiling â†’ Planning â†’ Generation â†’ XAI** pipeline
    
- Uses **Phase 0 knowledge base**
     
- Fully functional **end-to-end demo**
    
- Can be extended later with clustering, graph-based planning, and LLM synthesis

#### Terminal
C:\Users\hatta\Desktop\study\M2\Generative AI\Projet\Implementation\rag_phase1>python scripts\orchestrator.py
2025-12-22 13:43:45.740682: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-12-22 13:43:47.285663: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
WARNING:tensorflow:From C:\Users\hatta\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.

=== Recommended Content ===
[4] QUADRATIC EQUATIONS 87
which can be rewritten as
â€” b Â± Vb2 â€” 4ac

### ğŸ”¹ Phase 2 â€“ Recommendation Intelligence (Weeks 4â€“5)

#### schema_phase2.sql
-- Learners

CREATE TABLE learners (

Â  Â  id SERIAL PRIMARY KEY,

Â  Â  learning_style TEXT,

Â  Â  knowledge_level FLOAT,

Â  Â  embedding FLOAT8[],

Â  Â  created_at TIMESTAMP DEFAULT NOW()

);

-- Interactions

CREATE TABLE learner_interactions (

Â  Â  id SERIAL PRIMARY KEY,

Â  Â  learner_id INT REFERENCES learners(id),

Â  Â  chunk_id INT REFERENCES chunks(id),

Â  Â  interaction_type TEXT,

Â  Â  rating INT,

Â  Â  created_at TIMESTAMP DEFAULT NOW()

);

-- Recommendation logs (XAI + evaluation)

CREATE TABLE recommendations (

Â  Â  id SERIAL PRIMARY KEY,

Â  Â  learner_id INT REFERENCES learners(id),

Â  Â  chunk_id INT REFERENCES chunks(id),

Â  Â  content_score FLOAT,

Â  Â  collaborative_score FLOAT,

Â  Â  final_score FLOAT,

Â  Â  explanation TEXT,

Â  Â  created_at TIMESTAMP DEFAULT NOW()

);

#### 01_simulate_learners.py
import psycopg2

import random

import numpy as np

from sentence_transformers import SentenceTransformer

import os

import sys

 Ensure project root is in path

PROJECT_ROOT = os.path.dirname(os.path.dirname(__file__))

sys.path.insert(0, PROJECT_ROOT)

from config import DB_CONFIG

Load embedding model

model = SentenceTransformer("all-MiniLM-L6-v2")

styles = ["visual", "textual", "problem-solving"]

def knowledge_to_level(knowledge_score):

Â  Â  """Convert numeric knowledge (0-1) to textual level"""

Â  Â  if knowledge_score < 0.4:

Â  Â  Â  Â  return "beginner"

Â  Â  elif knowledge_score < 0.7:

Â  Â  Â  Â  return "intermediate"

Â  Â  else:

Â  Â  Â  Â  return "advanced"

 Connect to DB

conn = psycopg2.connect(**DB_CONFIG)

cur = conn.cursor()

 Ensure embedding column exists

cur.execute("""

ALTER TABLE learners

ADD COLUMN IF NOT EXISTS embedding FLOAT8[];

""")

conn.commit()

 Option 1: Insert new learners

for _ in range(15):

Â  Â  style = random.choice(styles)

Â  Â  knowledge_score = round(random.uniform(0.2, 0.9), 2)

Â  Â  level = knowledge_to_level(knowledge_score)

Â  Â  # Generate embedding

Â  Â  emb = model.encode([f"{style} learner math level {knowledge_score}"])[0]

Â  Â  cur.execute("""

Â  Â  Â  Â  INSERT INTO learners (learning_style, level, embedding)

Â  Â  Â  Â  VALUES (%s, %s, %s)

Â  Â  """, (style, level, emb.tolist()))

Option 2: Update existing learners (if you want to populate embeddings for them)

cur.execute("SELECT id, learning_style, level FROM learners WHERE embedding IS NULL")

for learner_id, style, level in cur.fetchall():

Â  Â  knowledge_score = {"beginner": 0.3, "intermediate": 0.55, "advanced": 0.8}[level]

Â  Â  emb = model.encode([f"{style} learner math level {knowledge_score}"])[0]

Â  Â  cur.execute(

Â  Â  Â  Â  "UPDATE learners SET embedding = %s WHERE id = %s",

Â  Â  Â  Â  (emb.tolist(), learner_id)

Â  Â  )

conn.commit()

conn.close()

print("Simulated learners inserted/updated with embeddings")

#### 02_log_interactions.py
import psycopg2

import random

import os

import sys

 Ensure project root is in path

PROJECT_ROOT = os.path.dirname(os.path.dirname(__file__))

sys.path.insert(0, PROJECT_ROOT)

from config import DB_CONFIG

conn = psycopg2.connect(**DB_CONFIG)

cur = conn.cursor()

cur.execute("SELECT id FROM learners")

learners = [r[0] for r in cur.fetchall()]

cur.execute("SELECT id FROM chunks")

chunks = [r[0] for r in cur.fetchall()]

for learner in learners:

Â  Â  for _ in range(random.randint(5, 15)):

Â  Â  Â  Â  cur.execute("""

Â  Â  Â  Â  Â  Â  INSERT INTO learner_interactions

Â  Â  Â  Â  Â  Â  (learner_id, chunk_id, interaction_type, rating)

Â  Â  Â  Â  Â  Â  VALUES (%s, %s, %s, %s)

Â  Â  Â  Â  """, (

Â  Â  Â  Â  Â  Â  learner,

Â  Â  Â  Â  Â  Â  random.choice(chunks),

Â  Â  Â  Â  Â  Â  random.choice(["view", "complete"]),

Â  Â  Â  Â  Â  Â  random.randint(1, 5)

Â  Â  Â  Â  ))

conn.commit()

conn.close()

print("Interactions simulated")

#### 03_hybrid_recommender.py
import psycopg2

import numpy as np

import faiss

from sentence_transformers import SentenceTransformer

import os

import sys

 Ensure project root is in path

PROJECT_ROOT = os.path.dirname(os.path.dirname(__file__))

sys.path.insert(0, PROJECT_ROOT)

from config import DB_CONFIG

ALPHA = 0.7

BETA = 0.3

model = SentenceTransformer("all-MiniLM-L6-v2")

index = faiss.read_index("../rag_phase0/faiss/index.faiss")

conn = psycopg2.connect(**DB_CONFIG)

cur = conn.cursor()

 pick a learner

cur.execute("SELECT id, embedding FROM learners LIMIT 1")

learner_id, learner_emb = cur.fetchone()

learner_emb = np.array(learner_emb).reshape(1, -1)

 content-based

D, I = index.search(learner_emb.astype("float32"), k=5)

for rank, idx in enumerate(I[0]):

Â  Â  content_score = float(1 / (1 + D[0][rank])) Â # cast to Python float

Â  Â  # collaborative score

Â  Â  cur.execute("""

Â  Â  Â  Â  SELECT COUNT(*)

Â  Â  Â  Â  FROM learner_interactions

Â  Â  Â  Â  WHERE chunk_id = %s

Â  Â  """, (int(idx),))

Â  Â  collaborative_score = float(cur.fetchone()[0]) Â # cast to Python float

Â  Â  final_score = float(ALPHA * content_score + BETA * collaborative_score) Â # cast

Â  Â  cur.execute("""

Â  Â  Â  Â  INSERT INTO recommendations

Â  Â  Â  Â  (learner_id, chunk_id, content_score, collaborative_score, final_score)

Â  Â  Â  Â  VALUES (%s, %s, %s, %s, %s)

Â  Â  """, (

Â  Â  Â  Â  learner_id,

Â  Â  Â  Â  int(idx),

Â  Â  Â  Â  content_score,

Â  Â  Â  Â  collaborative_score,

Â  Â  Â  Â  final_score

Â  Â  ))

conn.commit()

conn.close()

print("Hybrid recommendations stored")

#### 04_explain_ranking.py
import psycopg2

import os

import sys

 Ensure project root is in path

PROJECT_ROOT = os.path.dirname(os.path.dirname(__file__))

sys.path.insert(0, PROJECT_ROOT)

from config import DB_CONFIG

conn = psycopg2.connect(**DB_CONFIG)

cur = conn.cursor()

cur.execute("""

SELECT r.id, l.learning_style, r.content_score, r.collaborative_score

FROM recommendations r

JOIN learners l ON r.learner_id = l.id

WHERE r.explanation IS NULL

LIMIT 5

""")

rows = cur.fetchall()

for rid, style, cs, coll in rows:

Â  Â  explanation = (

Â  Â  Â  Â  f"This resource was recommended because it matches your {style} learning style "

Â  Â  Â  Â  f"(content similarity = {cs:.2f}) and was frequently useful to similar learners "

Â  Â  Â  Â  f"(collaborative score = {coll:.2f})."

Â  Â  )

Â  Â  cur.execute("""

Â  Â  Â  Â  UPDATE recommendations

Â  Â  Â  Â  SET explanation = %s

Â  Â  Â  Â  WHERE id = %s

Â  Â  """, (explanation, rid))

conn.commit()

conn.close()

print("Explanations generated")

#### 05_evaluate.py
import psycopg2

import os

import sys

 Ensure project root is in path

PROJECT_ROOT = os.path.dirname(os.path.dirname(__file__))

sys.path.insert(0, PROJECT_ROOT)

from config import DB_CONFIG

conn = psycopg2.connect(**DB_CONFIG)

cur = conn.cursor()

cur.execute("""

SELECT AVG(final_score)

FROM recommendations

""")

hybrid_avg = cur.fetchone()[0]

cur.execute("""

SELECT AVG(random())

FROM recommendations

""")

random_avg = cur.fetchone()[0]

print("Hybrid avg score:", hybrid_avg)

print("Random baseline:", random_avg)

### ğŸ”¹ Phase 3 â€“ Advanced XAI & Evaluation (Weeks 6â€“7)

#### 06_shap_analysis
import psycopg2

import numpy as np

import shap

import os

import sys

PROJECT_ROOT = os.path.dirname(os.path.dirname(__file__))

sys.path.insert(0, PROJECT_ROOT)

from config import DB_CONFIG

conn = psycopg2.connect(**DB_CONFIG)

cur = conn.cursor()

 Sample: fetch recommendations and learner embeddings

cur.execute("SELECT r.final_score, l.embedding FROM recommendations r JOIN learners l ON r.learner_id=l.id")

data = cur.fetchall()

X = np.array([row[1] for row in data]) Â # learner embeddings

y = np.array([row[0] for row in data]) Â # final_score

 Use a simple linear model as surrogate for SHAP

import xgboost as xgb

model = xgb.XGBRegressor()

model.fit(X, y)

explainer = shap.Explainer(model)

shap_values = explainer(X)

 Save top features for audit

np.save("shap_values.npy", shap_values.values)

print("SHAP analysis complete")

conn.close()

#### 07_evaluate_xai.py
import psycopg2

import os

import sys

PROJECT_ROOT = os.path.dirname(os.path.dirname(__file__))

sys.path.insert(0, PROJECT_ROOT)

from config import DB_CONFIG

conn = psycopg2.connect(**DB_CONFIG)

cur = conn.cursor()

cur.execute("SELECT AVG(final_score) FROM recommendations")

hybrid_avg = cur.fetchone()[0]

cur.execute("SELECT AVG(random()) FROM recommendations")

random_avg = cur.fetchone()[0]

Evaluate explanation length & richness

cur.execute("SELECT explanation FROM recommendations WHERE explanation IS NOT NULL")

explanations = cur.fetchall()

avg_length = sum(len(exp[0].split()) for exp in explanations) / len(explanations)

print("Hybrid avg score:", hybrid_avg)

print("Random baseline:", random_avg)

print("Avg explanation length (words):", avg_length)

conn.close()

#### 08_bias_audit
 scripts/08_bias_audit.py

import psycopg2

import os

import sys

 Add project root to path if needed

PROJECT_ROOT = os.path.dirname(os.path.dirname(__file__))

sys.path.insert(0, PROJECT_ROOT)

from config import DB_CONFIG Â # your database connection settings

 Connect to the database

conn = psycopg2.connect(**DB_CONFIG)

cur = conn.cursor()

Query: average recommendation scores by learning style

cur.execute("""

SELECT l.learning_style, AVG(r.final_score)

FROM recommendations r

JOIN learners l ON r.learner_id = l.id

GROUP BY l.learning_style

""")

bias_stats = cur.fetchall()

print("Avg final scores by learning style:", bias_stats)

  Close connection

cur.close()

conn.close()

####  xai_agent.py
def justify_recommendation(plan, profile, chunk_id, shap_values=None):

Â  Â  base_explanation = (

Â  Â  Â  Â  f"We recommend a {plan['recommendation_type']} for {plan['next_concept']} "

Â  Â  Â  Â  f"because your current knowledge score is {profile['knowledge'][plan['next_concept']]:.1f} "

Â  Â  Â  Â  f"and you prefer {profile['preferences'][0]}."

Â  Â  )

Â  Â  if shap_values is not None:

Â  Â  Â  Â  # Include top influencing features

Â  Â  Â  Â  top_features = ", ".join([f"feature_{i}" for i in shap_values[:3]])

Â  Â  Â  Â  base_explanation += f" Top influencing factors: {top_features}."

Â  Â  return base_explanation

# Full implementation

## ğŸ”¹ Step 1 â€” Consolidate Agents
From your description, the final integration should include **6 agents**:

1. **Profiling Agent**
    
    - **Input:** raw interaction logs
        
    - **Output:** structured `LearnerProfile`
        
    - **Implementation:** Use `rag_phase2/scripts/01_simulate_learners.py` as base. Add clustering / embedding logic for richer profiles.
        
    - **Tools:** `scikit-learn` (clustering), `SentenceTransformers` (embedding)
        
2. **Planning Agent**
    
    - **Input:** `LearnerProfile`, course objectives
        
    - **Output:** adaptive `LearningPath`
        
    - **Implementation:** Start with your `plan_learning_path()` logic from Phase 1. Extend it to handle a **graph of concepts** (prerequisites).
        
    - **Tools:** NetworkX (for concept graphs), or a custom adjacency matrix
        
3. **Orchestrator Agent**
    
    - **Input:** LearningPath + LearnerProfile
        
    - **Behavior:** Delegates tasks to **Content Generator** and **Recommendation Agent**, collects outputs, passes to **XAI**.
        
    - **Implementation:** Refactor your `orchestrator.py` to handle **task delegation** and maintain **state**. Consider an in-memory `dict` of learner progress.
        
    - **Tools:** Could integrate **LangGraph** later, but sequential Python calls are fine for now
        
4. **Content Generator Agent (RAG + LLM)**
    
    - **Input:** concept + learner profile
        
    - **Output:** generated content (explanation, example, quiz)
        
    - **Implementation:** Build on `generate_content()` from Phase 1. Ensure you retrieve multiple chunks from FAISS and format a prompt for your LLM.
        
    - **Tools:** `SentenceTransformers`, FAISS, OpenAI API (GPT-4)
        
5. **Recommendation Agent (Hybrid)**
    
    - **Input:** LearnerProfile + LearningPath node
        
    - **Output:** ranked list of existing resources
        
    - **Implementation:** Extend `03_hybrid_recommender.py` logic to **filter by concept** and include top-k selection.
        
    - **Tools:** PostgreSQL, FAISS, NumPy
        
6. **XAI Agent**
    
    - **Input:** recommendation + learner profile + optional SHAP values
        
    - **Output:** natural language explanation
        
    - **Implementation:** Combine `xai_agent.py` with `06_shap_analysis.py` results. Top features can be inserted into prompt for the LLM.
        
    - **Tools:** SHAP, LLM, possibly template prompts
---
## ğŸ”¹ Step 2 â€” Standardize the Data Flow
We need a clear **end-to-end flow**:

`raw interaction logs       â†“ Profiling Agent â†’ LearnerProfile       â†“ Planning Agent â†’ LearningPath (concept sequence)       â†“ Orchestrator â†’ dispatch tasks:       â”œâ”€> Content Generator â†’ generated content       â”œâ”€> Recommendation Agent â†’ existing resource ranking       â†“ XAI Agent â†’ explanation text       â†“ Final output â†’ user interface / report`

**Key integration points**:

- Orchestrator collects outputs and ensures the learner sees **both generated content and top recommended resources** with explanations.
    
- SHAP analysis can be **precomputed** and passed to XAI for feature importance.
---
## ğŸ”¹ Step 3 â€” Implementation Tips
1. **Shared config**
    
    - All agents should import a single `config.py` for DB connection.
        
    - Example:

    `from config import DB_CONFIG`
    
2. **Project structure refinement**
    Keep Phase 3 as the integration hub:

    `rag_phase3/ â”œâ”€ scripts/ â”‚  â”œâ”€ orchestrator.py  # central runner â”‚  â”œâ”€ profiling_agent.py â”‚  â”œâ”€ planning_agent.py â”‚  â”œâ”€ content_generator.py â”‚  â”œâ”€ recommendation_agent.py â”‚  â”œâ”€ xai_agent.py â”‚  â””â”€ shap_analysis.py`
    
3. **Task delegation (Orchestrator)**
    
    - Define simple functions to call other agents:

    `profile = profiling_agent.get_profile(learner_id) path = planning_agent.plan_path(profile) content = content_generator.generate(path.current, profile) recommendations = recommendation_agent.recommend(path.current, profile) explanation = xai_agent.justify(content, recommendations, profile)`
    
4. **SHAP integration**
    
    - Run `06_shap_analysis.py` once, save `shap_values.npy`.
        
    - Load in Orchestrator â†’ pass top features to XAI for **contextual explanations**.
        
5. **Testing the pipeline**
    
    - Start with **one learner** for end-to-end validation.
        
    - Print: content, recommended chunks, explanation.
---
## ğŸ”¹ Step 4 â€” Next Immediate Action
1. Refactor **Phase 3 scripts** to act as **individual agents** with callable functions.
    
2. Create `orchestrator.py` in Phase 3 to **tie all agents together**.
    
3. Ensure **SHAP values** are loaded and passed to XAI.
    
4. Run a single-learner end-to-end demo, print all outputs.
## Results

**note** all the inputs can be found inside the file profiling_agent.py
### Test 1
#### Input profile

Â  {

Â  Â  Â  Â  "learning_style": "visual",

Â  Â  Â  Â  "preferences": {

Â  Â  Â  Â  Â  Â  "modality": "visual",

Â  Â  Â  Â  Â  Â  "difficulty": "beginner",

Â  Â  Â  Â  Â  Â  "verbosity": "short"

Â  Â  Â  Â  },

Â  Â  Â  Â  "knowledge": {

Â  Â  Â  Â  Â  Â  "algebra": 0.2,

Â  Â  Â  Â  Â  Â  "quadratic_eq": 0.1,

Â  Â  Â  Â  Â  Â  "calculus": 0.0

Â  Â  Â  Â  },

Â  Â  Â  Â  "goal": "intuition",

Â  Â  Â  Â  "background": "science"

Â  Â  },
Â  Â  
#### Terminal output

C:\Users\hatta\Desktop\study\M2\Generative AI\Projet\Implementation\rag_phase3>python scripts/orchestrator.py
2025-12-24 10:20:47.665492: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-12-24 10:20:52.242078: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
WARNING:tensorflow:From C:\Users\hatta\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.

`torch_dtype` is deprecated! Use `dtype` instead!

[PROFILING AGENT]
  Simulated profile: {'id': 1, 'learning_style': 'visual', 'preferences': {'modality': 'visual', 'difficulty': 'beginner', 'verbosity': 'short'}, 'knowledge': {'algebra': 0.2, 'quadratic_eq': 0.1, 'calculus': 0.0}, 'goal': 'intuition', 'background': 'science'}

[PLANNING AGENT]
  Knowledge state: {'algebra': 0.2, 'quadratic_eq': 0.1, 'calculus': 0.0}
  Sorted concepts (low â†’ high mastery): ['calculus', 'quadratic_eq', 'algebra']
  Mode: exploit
  Reward received: 0
  â¡ Selected concept: calculus
  Planned horizon: ['calculus', 'algebra']

[ORCHESTRATOR]
  Step 1 / 3
  Current concept: calculus
  Plan history: ['calculus']
  Mode: exploit
  Backtrack flag: False
The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.

[RECOMMENDATION AGENT]
  Previous avg score: 0.0
  Current avg score: -0.02000000000000008
  Reward signal: -0.02000000000000008

=== STRATEGIC SUMMARY ===
Plan history: ['calculus']
Final reward: -0.02000000000000008
Total planning steps: 1

=== AGENT EXECUTION TRACE ===
profiling -> planning -> content -> recommendation -> xai

=== CONTENT ===
 The following is a list of the topics which are covered in this book.
1. Introduction to vectors and vectors in the plane.
2. Introduction to the coordinate system in the plane.
3. Introduction to the coordinate system in the three-dimensional space.
4. Introduction to the coordinate system in the four-dimensional space.
5. Introduction to the coordinate system in the n-dimensional space.
6. Introduction to the coordinate system in the n-dimensional space.
7. Introduction to the coordinate system in the n-dimensional space.
8. Introduction to the coordinate system in the n-dimensional space.
9. Introduction to the coordinate system in the n-dimensional space.
10. Introduction to the coordinate system in the n-dimensional space.
11. Introduction to the coordinate system in the n-dimensional space.
12. Introduction to the coordinate system in the n-dimensional space.
13. Introduction to the coordinate system in the n-dimensional space.
14. Introduction to the coordinate system in the n-dimensional space.
15. Introduction to the coordinate system in the n-dimensional space.
16. Introduction to the coordinate system in the n-dimensional space.
17. Introduction to the coordinate system in the n-dimensional space.
18. Introduction to the coordinate system in the n-dimensional space.
19

=== RECOMMENDATIONS ===
 [{'chunk_id': 3, 'final_score': -0.4}, {'chunk_id': 4, 'final_score': -0.30000000000000004}, {'chunk_id': 6, 'final_score': -5.551115123125783e-17}, {'chunk_id': 2, 'final_score': 0.6999999999999998}, {'chunk_id': 221, 'final_score': -0.10000000000000006}]

=== EXPLANATION ===
 The system recommended these resources for the concept 'calculus' because they align with the learner's preferences (modality: visual, difficulty: beginner), and match assumed pedagogical properties of the content, in addition to collaborative popularity signals.

1. Resource chunk 2 (combined relevance score = 0.70)
2. Resource chunk 6 (combined relevance score = -0.00)
3. Resource chunk 221 (combined relevance score = -0.10)

This explanation is based on a hybrid recommendation strategy combining content similarity and collaborative popularity.
### Test 2
#### Input profile
Â  Â {

Â  Â  Â  Â  "learning_style": "textual",

Â  Â  Â  Â  "preferences": {

Â  Â  Â  Â  Â  Â  "modality": "textual",

Â  Â  Â  Â  Â  Â  "difficulty": "intermediate",

Â  Â  Â  Â  Â  Â  "verbosity": "detailed"

Â  Â  Â  Â  },

Â  Â  Â  Â  "knowledge": {

Â  Â  Â  Â  Â  Â  "algebra": 0.7,

Â  Â  Â  Â  Â  Â  "quadratic_eq": 0.5,

Â  Â  Â  Â  Â  Â  "calculus": 0.2

Â  Â  Â  Â  },

Â  Â  Â  Â  "goal": "exam-prep",

Â  Â  Â  Â  "background": "engineering"

Â  Â  },
#### Output


C:\Users\hatta\Desktop\study\M2\Generative AI\Projet\Implementation\rag_phase3>python scripts/orchestrator.py
2025-12-24 10:24:47.571130: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-12-24 10:24:49.140367: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
WARNING:tensorflow:From C:\Users\hatta\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.

`torch_dtype` is deprecated! Use `dtype` instead!

[PROFILING AGENT]
  Simulated profile: {'id': 2, 'learning_style': 'textual', 'preferences': {'modality': 'textual', 'difficulty': 'intermediate', 'verbosity': 'detailed'}, 'knowledge': {'algebra': 0.7, 'quadratic_eq': 0.5, 'calculus': 0.2}, 'goal': 'exam-prep', 'background': 'engineering'}

[PLANNING AGENT]
  Knowledge state: {'algebra': 0.7, 'quadratic_eq': 0.5, 'calculus': 0.2}
  Sorted concepts (low â†’ high mastery): ['calculus', 'quadratic_eq', 'algebra']
  Mode: exploit
  Reward received: 0
  â¡ Selected concept: calculus
  Planned horizon: ['calculus', 'algebra']

[ORCHESTRATOR]
  Step 1 / 3
  Current concept: calculus
  Plan history: ['calculus']
  Mode: exploit
  Backtrack flag: False
The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.

[RECOMMENDATION AGENT]
  Previous avg score: 0.0
  Current avg score: -0.24000000000000005
  Reward signal: -0.24000000000000005

[ORCHESTRATOR]
  âš ï¸ Negative reward AFTER feedback, scheduling replanning

[PLANNING AGENT]
  Knowledge state: {'algebra': 0.7, 'quadratic_eq': 0.5, 'calculus': 0.2}
  Sorted concepts (low â†’ high mastery): ['calculus', 'quadratic_eq', 'algebra']
  Mode: exploit
  Reward received: -0.24000000000000005
  âš ï¸ BACKTRACKING TRIGGERED
  â†© Returning to previous concept: calculus
  Planned horizon: ['calculus', 'algebra']

[TRACE]
  â†© BACKTRACKING EVENT
  Returned to concept: calculus
  Trace so far: profiling -> planning -> content -> recommendation -> planning(backtrack)

[ORCHESTRATOR]
  Step 2 / 3
  Current concept: calculus
  Plan history: ['calculus', 'calculus']
  Mode: exploit
  Backtrack flag: True

=== STRATEGIC SUMMARY ===
Plan history: ['calculus', 'calculus']
Final reward: -0.24000000000000005
Total planning steps: 2

=== AGENT EXECUTION TRACE ===
profiling -> planning -> content -> recommendation -> planning(backtrack) -> xai

=== CONTENT ===
 Create a intermediate level, textual explanation with detailed verbosity about 'calculus' for a engineering student (goal: exam-prep).

Use the following sources:
X FOREWORD
motivated, especially when the possible topics are as attractive as vector
geometry.
In fact, for many years college courses in physics and engineering have
faced serious drawbacks in scheduling because they need simultaneously
some calculus and also some vector geometry. It is very unfortunate that the
most basic operations on vectors are introduced at present only in college.
They should appear at least as early as the second year of high school. I
cannot write here a text for elementary geometry (although to some extent
the parts on intuitive geometry almost constitute such a text), but I hope
that the present book will provide considerable impetus to lower considerably
the level at which vectors are introduced. Within some foreseeable future,
the topics covered in this book should in fact be the standard topics for the
second year of high school, so that the third and fourth years can be devoted
to calculus and linear algebra.
If only preparatory material for calculus is needed, many portions of
this book can be omitted, and attention should be directed to the rules of
arithmetic, linear equations (Chapter 2), quadratic

=== RECOMMENDATIONS ===
 [{'chunk_id': 2, 'final_score': 0.4999999999999999}, {'chunk_id': 6, 'final_score': -0.20000000000000007}, {'chunk_id': 4, 'final_score': -0.5}, {'chunk_id': 44, 'final_score': -0.8}, {'chunk_id': 46, 'final_score': -0.20000000000000007}]

=== EXPLANATION ===
 The system recommended these resources for the concept 'calculus' because they align with the learner's preferences (modality: textual, difficulty: intermediate), and match assumed pedagogical properties of the content, in addition to collaborative popularity signals.

1. Resource chunk 2 (combined relevance score = 0.50)
2. Resource chunk 6 (combined relevance score = -0.20)
3. Resource chunk 46 (combined relevance score = -0.20)

This explanation is based on a hybrid recommendation strategy combining content similarity and collaborative popularity.
### Test 3
#### Input Profile
Â {

Â  Â  Â  Â  "learning_style": "problem-solving",

Â  Â  Â  Â  "preferences": {

Â  Â  Â  Â  Â  Â  "modality": "problem-solving",

Â  Â  Â  Â  Â  Â  "difficulty": "advanced",

Â  Â  Â  Â  Â  Â  "verbosity": "short"

Â  Â  Â  Â  },

Â  Â  Â  Â  "knowledge": {

Â  Â  Â  Â  Â  Â  "algebra": 0.9,

Â  Â  Â  Â  Â  Â  "quadratic_eq": 0.8,

Â  Â  Â  Â  Â  Â  "calculus": 0.6

Â  Â  Â  Â  },

Â  Â  Â  Â  "goal": "fast-review",

Â  Â  Â  Â  "background": "math"

Â  Â  }
#### Output

C:\Users\hatta\Desktop\study\M2\Generative AI\Projet\Implementation\rag_phase3>python scripts/orchestrator.py
2025-12-24 10:29:06.190610: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-12-24 10:29:07.757235: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
WARNING:tensorflow:From C:\Users\hatta\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.

`torch_dtype` is deprecated! Use `dtype` instead!

[PROFILING AGENT]
  Simulated profile: {'id': 3, 'learning_style': 'problem-solving', 'preferences': {'modality': 'problem-solving', 'difficulty': 'advanced', 'verbosity': 'short'}, 'knowledge': {'algebra': 0.9, 'quadratic_eq': 0.8, 'calculus': 0.6}, 'goal': 'fast-review', 'background': 'math'}

[PLANNING AGENT]
  Knowledge state: {'algebra': 0.9, 'quadratic_eq': 0.8, 'calculus': 0.6}
  Sorted concepts (low â†’ high mastery): ['calculus', 'quadratic_eq', 'algebra']
  Mode: exploit
  Reward received: 0
  â¡ Selected concept: calculus
  Planned horizon: ['calculus', 'algebra']

[ORCHESTRATOR]
  Step 1 / 3
  Current concept: calculus
  Plan history: ['calculus']
  Mode: exploit
  Backtrack flag: False
  âœ… Mastery threshold reached or max steps exceeded.
The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.

[RECOMMENDATION AGENT]
  Previous avg score: 0.0
  Current avg score: -0.07000000000000003
  Reward signal: -0.07000000000000003

[ORCHESTRATOR]
  âš ï¸ Negative reward AFTER feedback, scheduling replanning

[PLANNING AGENT]
  Knowledge state: {'algebra': 0.9, 'quadratic_eq': 0.8, 'calculus': 0.6}
  Sorted concepts (low â†’ high mastery): ['calculus', 'quadratic_eq', 'algebra']
  Mode: exploit
  Reward received: -0.07000000000000003
  âš ï¸ BACKTRACKING TRIGGERED
  â†© Returning to previous concept: calculus
  Planned horizon: ['calculus', 'algebra']

[TRACE]
  â†© BACKTRACKING EVENT
  Returned to concept: calculus
  Trace so far: profiling -> planning -> content -> recommendation -> planning(backtrack)

[ORCHESTRATOR]
  Step 2 / 3
  Current concept: calculus
  Plan history: ['calculus', 'calculus']
  Mode: exploit
  Backtrack flag: True
  âœ… Mastery threshold reached or max steps exceeded.

=== STRATEGIC SUMMARY ===
Plan history: ['calculus', 'calculus']
Final reward: -0.07000000000000003
Total planning steps: 2

=== AGENT EXECUTION TRACE ===
profiling -> planning -> content -> recommendation -> planning(backtrack) -> xai

=== CONTENT ===
 Create a advanced level, problem-solving explanation with short verbosity about 'calculus' for a math student (goal: fast-review).

Use the following sources:
The present book is intended as a text in basic mathematics. As such,
it can have multiple use: for a one-year course in the high schools during the
third or fourth year (if possible the third, so that calculus can be taken
during the fourth year); for a complementary reference in earlier high school
grades (elementary algebra and geometry are covered); for a one-semester
course at the college level, to review or to get a firm foundation in the basic
mathematics necessary to go ahead in calculus, linear algebra, or other topics.
Years ago, the colleges used to give courses in â€œcollege algebraâ€ and
other subjects which should have been covered in high school. More recently,
such courses have been thought unnecessary, but some experiences I have had
show that they are just as necessary as ever. What is happening is that the
colleges are getting a wide variety of students from high schools, ranging
from exceedingly well-prepared ones who have had a good first course in
calculus, down to very poorly prepared ones. This latter group includes both
adults who

=== RECOMMENDATIONS ===
 [{'chunk_id': 3, 'final_score': -0.25}, {'chunk_id': 4, 'final_score': -0.25}, {'chunk_id': 6, 'final_score': 0.04999999999999993}, {'chunk_id': 2, 'final_score': 0.5499999999999999}, {'chunk_id': 94, 'final_score': -0.45000000000000007}]

=== EXPLANATION ===
 The system recommended these resources for the concept 'calculus' because they align with the learner's preferences (modality: problem-solving, difficulty: advanced), and match assumed pedagogical properties of the content, in addition to collaborative popularity signals.

1. Resource chunk 2 (combined relevance score = 0.55)
2. Resource chunk 6 (combined relevance score = 0.05)
3. Resource chunk 3 (combined relevance score = -0.25)

This explanation is based on a hybrid recommendation strategy combining content similarity and collaborative popularity.
### Test 4
#### Input profile
Â  Â  Â  Â  9999: { Â  Â  Â  Â  Â # custom profile to force backtracking

Â  Â  Â  Â  Â  Â  "learning_style": "visual",

Â  Â  Â  Â  Â  Â  "preferences": {"modality": "problem-solving", "difficulty": "advanced", "verbosity": "short"},

Â  Â  Â  Â  Â  Â  "knowledge": {"algebra": 0.0, "quadratic_eq": 0.0, "calculus": 0.0},

Â  Â  Â  Â  Â  Â  "goal": "intuition",

Â  Â  Â  Â  Â  Â  "background": "science"

Â  Â  Â  Â  }
#### Output
C:\Users\hatta\Desktop\study\M2\Generative AI\Projet\Implementation\rag_phase3>python scripts/orchestrator.py
2025-12-24 10:32:51.009371: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-12-24 10:32:52.644077: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
WARNING:tensorflow:From C:\Users\hatta\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.

`torch_dtype` is deprecated! Use `dtype` instead!

[PROFILING AGENT]
  Simulated profile: {'id': 9999, 'learning_style': 'visual', 'preferences': {'modality': 'problem-solving', 'difficulty': 'advanced', 'verbosity': 'short'}, 'knowledge': {'algebra': 0.0, 'quadratic_eq': 0.0, 'calculus': 0.0}, 'goal': 'intuition', 'background': 'science'}

[PLANNING AGENT]
  Knowledge state: {'algebra': 0.0, 'quadratic_eq': 0.0, 'calculus': 0.0}
  Sorted concepts (low â†’ high mastery): ['algebra', 'quadratic_eq', 'calculus']
  Mode: exploit
  Reward received: 0
  â¡ Selected concept: algebra
  Planned horizon: ['algebra']

[ORCHESTRATOR]
  Step 1 / 3
  Current concept: algebra
  Plan history: ['algebra']
  Mode: exploit
  Backtrack flag: False
The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.

[RECOMMENDATION AGENT]
  Previous avg score: 0.0
  Current avg score: -0.08000000000000004
  Reward signal: -0.08000000000000004

[ORCHESTRATOR]
  âš ï¸ Negative reward AFTER feedback, scheduling replanning

[PLANNING AGENT]
  Knowledge state: {'algebra': 0.0, 'quadratic_eq': 0.0, 'calculus': 0.0}
  Sorted concepts (low â†’ high mastery): ['algebra', 'quadratic_eq', 'calculus']
  Mode: exploit
  Reward received: -0.08000000000000004
  âš ï¸ BACKTRACKING TRIGGERED
  â†© Returning to previous concept: algebra
  Planned horizon: ['algebra']

[TRACE]
  â†© BACKTRACKING EVENT
  Returned to concept: algebra
  Trace so far: profiling -> planning -> content -> recommendation -> planning(backtrack)

[ORCHESTRATOR]
  Step 2 / 3
  Current concept: algebra
  Plan history: ['algebra', 'algebra']
  Mode: exploit
  Backtrack flag: True

=== STRATEGIC SUMMARY ===
Plan history: ['algebra', 'algebra']
Final reward: -0.08000000000000004
Total planning steps: 2

=== AGENT EXECUTION TRACE ===
profiling -> planning -> content -> recommendation -> planning(backtrack) -> xai

=== CONTENT ===
 The present book is intended as a text in basic mathematics. As such,
it can have multiple use: for a one-year course in the high schools during the
third or fourth year (if possible the third, so that calculus can be taken
during the fourth year); for a complementary reference in earlier high school
grades (elementary algebra and geometry are covered); for a one-semester
course at the college level, to review or to get a firm foundation in the basic
mathematics necessary to go ahead in calculus, linear algebra, or other topics.
Years ago, the colleges used to give courses in â€œcollege algebraâ€ and
other subjects which should have been covered in high school. More recently,
such courses have been thought unnecessary, but some experiences I have had
show that they are just as necessary as ever. What is happening is that the
colleges are getting a wide variety of students from high schools, ranging
from exceedingly well-prepared ones who have had a good first course in
calculus, down to very poorly prepared ones. This latter group includes both
adults who return to college after several yearsâ€™ absence in order to improve
their technical education, and students from the high schools who were not
adequately taught

=== RECOMMENDATIONS ===
 [{'chunk_id': 73, 'final_score': -0.3500000000000001}, {'chunk_id': 206, 'final_score': -0.30000000000000004}, {'chunk_id': 31, 'final_score': -5.551115123125783e-17}, {'chunk_id': 296, 'final_score': 0.14999999999999994}, {'chunk_id': 334, 'final_score': 0.09999999999999995}]

=== EXPLANATION ===
 The system recommended these resources for the concept 'algebra' because they align with the learner's preferences (modality: problem-solving, difficulty: advanced), and match assumed pedagogical properties of the content, in addition to collaborative popularity signals.

1. Resource chunk 296 (combined relevance score = 0.15)
2. Resource chunk 334 (combined relevance score = 0.10)
3. Resource chunk 31 (combined relevance score = -0.00)

This explanation is based on a hybrid recommendation strategy combining content similarity and collaborative popularity.

# Assumptions made and borhen's work

## Progress so far and what is missing  

Excellent work â€” what you have now is **not a toy**, it is a **credible experimental implementation** ğŸ‘  
What youâ€™re asking for now is exactly the _right academic step_:  
ğŸ‘‰ **mapping theory â†’ what is actually satisfied by the code â†’ what is only partially satisfied or future work**.

Below is a **clean, defensible analysis** you can directly turn into **notes or a report section** for your colleague.

---

### 1ï¸âƒ£ Agents & Data Flow â€” Theory vs Implementation

Iâ€™ll go agent by agent.

---

#### 1. Profiling Agent

##### ğŸ“š **Theory**

> Receives raw student interaction data. Uses clustering algorithms and LLM embeddings to model learner knowledge, preferences, and learning style.

##### âœ… **Satisfied in Code**

âœ” Uses **SentenceTransformers** (`all-MiniLM-L6-v2`)  
âœ” Learner profile includes:

- learning style
    
- preferences
    
- knowledge per concept
    
- goal & background  
    âœ” Profile is injected into **all downstream agents**  
    âœ” Supports **multiple simulated learners**, including adversarial one (`9999`)
    

`state["profile"] = get_profile(state["learner_id"])`

âœ” Knowledge â†’ difficulty mapping logic exists (`knowledge_to_level`)

---

##### âš ï¸ **Partially Satisfied**

âœ– No **online update** from real interaction logs  
âœ– No **KMeans actually applied at runtime** (present but unused)

---

##### ğŸ“ **Conclusion**

> The Profiling Agent is **structurally correct** and **embedding-aware**, but currently operates in a **simulated / offline profiling mode**, suitable for prototyping and controlled experiments.

---

#### 2. Planning Agent

##### ğŸ“š **Theory**

> Uses graph search and reasoning to generate an adaptive learning path, with backtracking.

---

##### âœ… **Satisfied in Code**

âœ” Explicit **prerequisite graph** (`PREREQUISITES`)  
âœ” Adaptive planning based on:

- learner knowledge
    
- reward feedback  
    âœ” **Îµ-greedy exploration vs exploitation**  
    âœ” **Backtracking implemented**  
    âœ” **Backtracking safety valve** (critical)
    

`if state.get("reward", 0) < -0.05 and state["plan_history"]:     backtrack = True`

âœ” Planning trace logged  
âœ” Multi-step horizon (`generate_plan`)

---

##### âš ï¸ **Partially Satisfied**

âœ– No heuristic cost function (e.g. A*)  
âœ– Planning depth fixed  
âœ– No explicit learning objective graph

---

##### ğŸ“ **Conclusion**

> The Planning Agent **implements adaptive reasoning with feedback and recovery**, fulfilling the core theoretical claim, though advanced search algorithms are left as future extensions.

---

#### 3. Orchestrator Agent (Central Coordinator)

##### ğŸ“š **Theory**

> Central hub managing state, delegating tasks, handling loops.

---

##### âœ… **Fully Satisfied**

âœ” Central shared state  
âœ” Dynamic agent routing  
âœ” Cyclic execution  
âœ” Reward-driven replanning  
âœ” Backtracking detection  
âœ” Execution trace (`state_trace.py`)  
âœ” Safety guard against infinite loops

`TASK_GRAPH = {     "profiling": ["planning"],     "planning": ["content", "recommendation"],     ... }`

---

##### ğŸ“ **Conclusion**

> The Orchestrator **fully satisfies the theoretical role**, even without LangGraph.  
> This is a **custom deterministic equivalent**, which is perfectly acceptable academically.

---

#### 4. Content Generator Agent (LLM + RAG)

##### ğŸ“š **Theory**

> Uses LLM & RAG to generate context-aware learning resources.

---

##### âœ… **Fully Satisfied**

âœ” SentenceTransformer embeddings  
âœ” FAISS semantic retrieval  
âœ” PostgreSQL chunk store  
âœ” Retrieved context grounding  
âœ” Prompt engineering with constraints  
âœ” **Critical fix: retrieved-text cleaning**  
âœ” Deterministic generation (`temperature=0`)

`retrieved_texts â†’ cleaned_texts â†’ prompt â†’ LLM`

---

##### ğŸ“ **Conclusion**

> This is a **textbook RAG pipeline**, cleanly implemented and experimentally sound.

---

#### 5. Recommendation Agent (Hybrid Filtering)

##### ğŸ“š **Theory**

> Combines collaborative + content-based filtering.

---

##### âœ… **Satisfied**

âœ” FAISS similarity search  
âœ” Collaborative score from interactions  
âœ” Content alignment bonuses  
âœ” Hybrid scoring formula  
âœ” Explicit logging to DB

`final_score = ALPHA + BETA * collab_score + bonuses`

âœ” Controlled randomness (global seed)

---

##### âš ï¸ **Partially Satisfied**

âœ– Collaborative data is sparse / simulated  
âœ– ALPHA deliberately negative (experimental)

---

##### ğŸ“ **Conclusion**

> The Recommendation Agent **faithfully implements the hybrid model**, with tunable parameters clearly exposed for experimentation.

---

#### 6. XAI (Explainable AI) Agent

##### ğŸ“š **Theory**

> Generates human-understandable justifications; supports auditing.

---

##### âœ… **Satisfied**

âœ” Natural language explanations  
âœ” Uses learner profile & ranking  
âœ” Faithful to recommendation logic  
âœ” SHAP computed offline  
âœ” Bias audit script exists

`06_shap_analysis.py 07_evaluate_xai.py 08_bias_audit.py`

---

##### âš ï¸ **Partially Satisfied**

âœ– SHAP not injected into natural language explanations (by design)  
âœ– No real-time explanation adaptation

---

##### ğŸ“ **Conclusion**

> XAI is **correctly layered**:
> 
> - human-facing explanations are LLM-based
>     
> - developer-facing audit uses SHAP  
>     This matches modern XAI best practice.
>     

---

### 2ï¸âƒ£ LLM & RAG Pipeline â€” Satisfaction Check

|Pipeline Step|Status|
|---|---|
|Input Query|âœ… Structured, profile-aware|
|Retrieval (FAISS)|âœ…|
|Context Cleaning|âœ…|
|LLM Grounded Generation|âœ…|
|Output Routing|âœ…|

âœ” Deterministic  
âœ” Reproducible  
âœ” Auditable

---

### 3ï¸âƒ£ Justification of Core Technical Choices â€” Reality Check

#### **Turbo is not Free ya borhen

 ğŸ“Œ Summary

|Access Method|Free?|Notes|
|---|---|---|
|**OpenAI API (GPT-4 Turbo)**|âŒ Not permanently|Billed per token; no free tier|
|**Intro API credits**|âœ”ï¸ Temporarily|Use credits to call GPT-4 Turbo until credits run out|
|**Other OpenAI models via API** (like GPT-3.5)|âœ”ï¸ cheaper or usable with credits|Lower cost options for dev/testing|
|**Chat (Copilot/Bing)**|âœ”ï¸ Chat use free|But _not API access_|

|Component|Status|
|---|---|
|GPT-4 Turbo|âŒ (Replaced by Qwen for local inference)|
|Open-source LLM|âœ…|
|Self-explaining prompts|âœ…|
|SHAP auditing|âœ…|
|LangGraph|âŒ (Custom orchestration instead)|
|FAISS + PostgreSQL|âœ…|

ğŸ‘‰ **Important academic note**:  
Using **Qwen locally** instead of GPT-4 is a **strength**, not a weakness, for reproducibility.

---

### 4ï¸âƒ£ Final High-Level Verdict (What You Can Say Confidently)

âœ… **What is fully satisfied**

- Multi-agent architecture
    
- Adaptive planning with backtracking
    
- LLM + RAG content generation
    
- Hybrid recommendation
    
- Explainability + bias auditing
    
- Deterministic, reproducible experiments
    

âš ï¸ **What is intentionally simplified**

- Real-time learner modeling
    
- Advanced graph search
    
- Online preference updates
    
- Production-scale orchestration framework
    

---

#### ğŸ§  One-sentence summary you can reuse

> _The proposed architecture is faithfully implemented at a functional level, validating the core multi-agent, RAG-based, and explainable design principles, while deliberately simplifying large-scale deployment and online learning aspects for experimental clarity._

If you want, next I can:

- Turn this into a **2â€“3 page report section**
    
- Rewrite it as **slides**
    
- Help you answer **â€œwhat are the limitations?â€** questions in defense style
## We need a better user profile

Looking closely at your codebase, hereâ€™s what I can tell about the **profile** and **user input**:

---

### 1ï¸âƒ£ Is there a real â€œprofileâ€?

- The **ProfilingAgent** calls `get_profile(learner_id)` from `profiling_agent.py`.
    
- `get_profile()` does two things:
    
    1. Fetches **historical embeddings** of the learner from the database (`learner_interactions`).
        
    2. Fetches **static learner info** (learning style, level) from the `learners` table.
        
- Then it constructs a `profile` dictionary:

`profile = {     "id": learner_id,     "learning_style": style,     "preferences": [style],     "knowledge": {         "math": knowledge_score,     } }`

âœ… So there **is a profile**, but it is **entirely inferred from past data**, and very minimal:

- Only **one knowledge metric** (`math`)
    
- **Learning style** is a single string
    
- **Preferences** is just a list containing the style

No additional explicit data about the learner (interests, goals, or custom input) is stored.

---

### 2ï¸âƒ£ How the system uses the profile

- **PlanningAgent** uses `profile["knowledge"]` to pick next concept.
    
- **ContentAgent** and **RecommendationAgent** use `profile["preferences"][0]` (learning style) to generate or rank content.
    
- **XAIAgent** explains recommendations using `profile` fields.

So the profile is **lightweight** and mainly **drives content personalization** based on:

- One knowledge score
    
- Learning style (visual, textual, problem-solving)

---

### 3ï¸âƒ£ Where user input exists (or doesnâ€™t)

- **There is no explicit user input** in the system besides the learner ID you pass to the Orchestrator:

`result = orch.run(learner_id=1)`

- All adaptation is **automatic** based on embeddings, historical interactions, and DB-stored static info.
    
- The learner does **not select concepts, difficulty, or preferred resource types**.

---

### 4ï¸âƒ£ Key insight

- The â€œprofileâ€ exists but is:
    
    - **Automatically generated**
        
    - **Minimal and static** (1 knowledge metric, 1 style)
        
    - **Derived from historical data**, not from real-time learner input
        
- So currently, the system behaves like the description you gave: **input is implicit**, derived from past interactions, not manual or interactive.

---

If you want, I can **map exactly where to inject real-time explicit user input** (like selecting interest, difficulty, or preferred content type) **without rewriting the agents much**, so the profile becomes more meaningful.

Do you want me to do that next?

## We need to classify the pdf's chunks using bert

Excellent instinct â€” youâ€™ve just identified **the main conceptual weakness** of your current system, and fixing it will make your project jump from _â€œworksâ€_ to _â€œactually makes sense academicallyâ€_.
Iâ€™ll answer in **three layers**:

1. **What is missing conceptually (and why youâ€™re right)**
    
2. **What is the minimal, correct architectural fix**
    
3. **Concrete code-level changes (keeping ~90% of your code intact)**

Iâ€™ll be very explicit about _where_ things go and _why_, so you can justify it in a report or defense.

---

### 1ï¸âƒ£ You are 100% right: chunks are currently â€œblindâ€

Right now, your chunks have **no semantic attributes**, only:

- text
    
- embedding
    
- popularity (interaction count)

So the system:

- âŒ does **not know** if a chunk is _algebra vs calculus_
    
- âŒ does **not know** if it is _hard vs easy_
    
- âŒ does **not know** if it is _long vs short_
    
- âŒ does **not know** if it is _theoretical vs problem-solving_

That means:

- Recommendation is **learner-centric**, not **content-aware**
    
- Planning selects _concepts_, but recommendations ignore them
    
- Personalization bonuses are **global**, not content-specific

Your intuition is correct:

> _â€œFor the system to be useful, it must understand the content of the PDF itself.â€_

---

### 2ï¸âƒ£ The correct mental model (very important)

You do **NOT** need a big LLM for this.

You need **content annotation**, not generation.

#### What each chunk SHOULD have

Each chunk should have **lightweight semantic labels**, for example:

`{   "concept": "calculus",   "difficulty": "advanced",   "modality": "problem-solving",   "length": "long" }`

These labels are then:

- compared with the **learner profile**
    
- compared with the **current planned concept**
    
- used as **multipliers**, not replacements, in scoring

This is exactly how industrial recommender systems work.

---

### 3ï¸âƒ£ Minimal, clean upgrade strategy (no rewrite)

Weâ€™ll do this in **three small steps**:

---

#### STEP A â€” Add semantic metadata to chunks (once, offline)

##### 1ï¸âƒ£ Database change (small)

Add columns to your `chunks` table:

`ALTER TABLE chunks ADD COLUMN concept TEXT, ADD COLUMN difficulty TEXT, ADD COLUMN modality TEXT, ADD COLUMN length TEXT;`

This does **not** break anything.

---

##### 2ï¸âƒ£ Automatic chunk labeling (encoder-based, fast, cheap)

You were right to think of **BERT-like encoders**.
Use a **zero-shot classifier**, not a generator.

Example model (very common, defendable academically):

`facebook/bart-large-mnli`

This can classify text into labels **without training**.

###### New script (run once)

ğŸ“ `scripts/annotate_chunks.py`

`from transformers import pipeline import psycopg2 from config import DB_CONFIG  classifier = pipeline("zero-shot-classification", model="facebook/bart-large-mnli")  CANDIDATES = {     "concept": ["algebra", "quadratic_eq", "calculus", "physics", "chemistry"],     "difficulty": ["beginner", "intermediate", "advanced"],     "modality": ["theoretical", "problem-solving", "visual"],     "length": ["short", "medium", "long"] }  conn = psycopg2.connect(**DB_CONFIG) cur = conn.cursor()  cur.execute("SELECT id, text FROM chunks") rows = cur.fetchall()  for chunk_id, text in rows:     labels = {}     for key, choices in CANDIDATES.items():         result = classifier(text[:512], choices)         labels[key] = result["labels"][0]      cur.execute("""         UPDATE chunks         SET concept=%s, difficulty=%s, modality=%s, length=%s         WHERE id=%s     """, (         labels["concept"],         labels["difficulty"],         labels["modality"],         labels["length"],         chunk_id     ))  conn.commit() conn.close()`

âœ… This gives your system **real semantic understanding**
âœ… No training
âœ… Explainable
âœ… Industry-standard

---

#### STEP B â€” Use chunk metadata in recommendation scoring (tiny change)

ğŸ“ **File:** `scripts/recommendation_agent.py`

##### Add this inside the recommendation loop

`cur.execute("""     SELECT concept, difficulty, modality     FROM chunks     WHERE id=%s """, (int(idx),))  chunk_meta = cur.fetchone() if chunk_meta:     chunk_concept, chunk_difficulty, chunk_modality = chunk_meta else:     chunk_concept, chunk_difficulty, chunk_modality = None, None, None`

---

##### Add **concept alignment bonus** (this is the missing link!)

`concept_bonus = 0.0 if chunk_concept == next_concept:     concept_bonus = 0.25`

---

##### Add **difficulty alignment bonus**

`difficulty_match_bonus = 0.0 if chunk_difficulty == profile["preferences"]["difficulty"]:     difficulty_match_bonus = 0.1`

---

##### Final score becomes

`final_score = (     ALPHA * content_score     + BETA * collab_score     + style_bonus     + difficulty_bonus     + concept_bonus     + difficulty_match_bonus )`

âš ï¸ Notice:

- You did NOT remove anything
    
- You only **added signals**
    
- The system remains hybrid & explainable

---

#### STEP C â€” Update XAI explanation (1 small paragraph)

ğŸ“ **File:** `xai_agent.py`

Change explanation text slightly:

`explanation = (     f"The system recommended these resources for the concept '{next_concept}' "     f"because they semantically match the target concept, "     f"align with the learner's profile "     f"(modality: {prefs['modality']}, difficulty: {prefs['difficulty']}), "     f"and are frequently interacted with by other learners.\n\n" )`

Now your explanation is **fully truthful**.

---

### 4ï¸âƒ£ Why this is academically strong (use this in defense)

You can now say:

> â€œThe system automatically annotates document fragments using a zero-shot semantic classifier. These content attributes are then aligned with the learner profile and the planning output to compute a composite relevance score.â€

This shows:

- NLP understanding
    
- Hybrid recommendation
    
- Planningâ€“recommendation coupling
    
- Explainability
    
- No hand-waving

---

### 5ï¸âƒ£ One-sentence killer summary

> The system evolves from learner-only similarity to concept-aware recommendation by automatically labeling content chunks and aligning them with the planned learning objective and learner preferences.

---

If you want next, I can:

- ğŸ” Add **debug prints showing why a chunk was chosen**
    
- ğŸ“Š Visualize score components per chunk
    
- ğŸ§  Replace zero-shot with a **fast rules+embedding hybrid**
    
- ğŸ§ª Make this deterministic for evaluation

Just tell me which direction you want to go.

